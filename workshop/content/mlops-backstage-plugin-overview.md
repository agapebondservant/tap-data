### Exploring the Backstage ML Portal

#### Launch Jupyter notebook from ML Portal
TAP's **Tanzu Developer Portal** is a customizable portal that can be extended to incorporate different **Backstage plugins**.
**Backstage** is an opensource framework that teams can use to configure a unified portal for all of their resources - 
infrastructure, services and documentation.

(<font color="red">NOTE:</font> For more information on **Backstage**, see the <a href="https://backstage.io/docs/overview/what-is-backstage" target="_blank">official docs<a/>.)

An example of a Backstage plugin extension is the **Backstage ML Portal**.
With this portal, data scientists can discover different Data / ML tools, pipelines and platforms that have been previously made available to them 
by a **Platform Operator**.
They can also use it to connect their Jupyter notebooks (and other similar IDEs) to the datasources or endpoints of their choosing.

Navigate to the **ML Panel**:
```dashboard:open-url
url: {{ ingress_protocol }}://tap-gui.tanzudatatap.com/mlworkflows
```

You can see tabs for different categories of services: **Data**, **Models**, **Pipelines**, **Clusters** and **Experiments**.

Thanks to **Backstage Components**, the actual items on each tab are configurable.
Where appropriate, users can add, remove or update tiles by simply navigating to the **Catalog** page:
```dashboard:open-url
url: {{ ingress_protocol }}://tap-gui.tanzudatatap.com/catalog
```

From there, they can make changes to the underlying YAML config by clicking on the **mltools-metadata** component -> "View Source".
The underlying metadata file from GitHub is displayed, and may be edited as appropriate by authorized users.

<font color="red"><b>NOTE: A separate workshop will go into the plugin configuration in more detail.</b></font>

Back on the **ML Panel**, click on the **Data** tab, and click **CONSOLE** on the Greenplum tile. 
The **Greenplum Command Center** is displayed.
Generally, the **CONSOLE** button links to any existing management console UI - or any other relevant GUI - that has been set up for access by the user.
If no console has been set up, then the **CONSOLE** will not be displayed.

Next, click on **CONNECT** tab, and click on the _copy_ icon for the displayed **ServiceBinding**.
**ServiceBindings** are a Kubernetes spec that can be used to connect to services without having to tamper with sensitive credentials.

For this demo, we will use our copied clipboard to connect to this Greenplum instance.
Click on the **Experiments** tab, and click on **CONSOLE** on the Jupyter tile.

Login to the JupyterLab environment - copy the **username** below  (password: **Vmware1!**):
```copy
{{session_namespace}}
```

There should be a templated notebook available - **connect-template.ipynb**.
Click on the notebook to launch it.
Then:
* Launch a new cell (hover underneath the existing cell for the "Click to add a cell" link to appear);
* Copy all the lines in the section entitled `# For Greenplum` to the new cell;
* Replace **bkstg-xxx-name-of-service-binding** with the value of the **ServiceBinding** just copied;
* Uncomment the content (using **Cmd + /** on Mac or **Ctrl + /** on Windows); and 
* Run the cell (using **Cmd + Enter** on Mac or **Ctrl + Enter** on Windows). 

<font color="red"><b>NOTE: The first line contains a "pip install" function, which may require a restart of the kernel to take effect. 
If you receive an error when running the notebook for the first time, simply click on "Restart the kernel" icon in the top panel
(hover over each icon to locate the right one).
</b></font>

Data from the Greenplum query should be displayed in a <a href="https://pandas.pydata.org/" target="_blank">pandas</a> **DataFrame**.

#### Experiment with postgresml LLM queries (Generative AI)
With **Tanzu Greenplum / Tanzu Postgres**, developers are able to take advantage of an exciting in-database capability for Generative AI: 
the **postgresml** extension. **Postgresml** provides the ability to build, train and predict with many different LLM, NLP and other ML workflows 
entirely within the database. 

In the case of Generative AI which uses LLMs (Large Language Models), 
inference results are generated by supplying the models with raw, natural language inputs (called **prompts**). 
**Postgresml** also supports the ability to convert the user's **prompts** into the right encoding format that LLM architectures need to use behind the scenes. 
It does this by chunking the inputs into **tokens**, and using an **Embedding Model** 
to transform the tokens into semantically meaningful, vectorized encodings called **embeddings**.
Both **Tanzu Greenplum** and **Tanzu Postgres** also support the **pgvector** extension, a plugin which is used 
to create database **indexes** (like _**HNSW**_-based indexes) for storing and retrieving the **embeddings** in a more efficient, resilient, scalable way.
Databases that are able to work with such semantic embeddings are called **vector databases**. **Postgresml** uses **pgvector** under the hood.

<div style="text-align: left; justify-content: left; align-items: center; width: 80%; margin-bottom: 20px; font-size: small">
    <img style="float: left; width: 20%; max-width: 20%; margin: 0 10px 0 0" src="images/mlops-tip.png"> 
    <b>PostgresML for In-Database AI</b><br/>
    For more information about PostgresML and how it compares to popular Python tech stacks for similar tasks, see <a href="https://postgresml.org/docs/" target="_blank">the documentation</a>.
</div>
<div style="clear: left;"></div>

Let's play with it briefly here. (More background will be covered in a separate workshop.)

* Back on the **ML Panel**, click on the **Data** tab;
* On the **Postgres** tile, click on the **CONNECT** tab, and click on the _copy_ icon for the **ServiceBinding** that includes _custom-user_
  (this will be the database binding that we leverage as our **Vector Database**);
* Launch a new cell (hover underneath the last cell for the "Click to add a cell" link to appear);
* Copy all the lines in the section entitled `# For Generative AI` to the new cell;
* Replace **bkstg-xxx-name-of-pgml1-svcbind-binding** with the **ServiceBinding** just copied;
* Uncomment the content (using **Cmd + /** on Mac or **Ctrl + /** on Windows); and
* Run the cell (using **Cmd + Enter** on Mac or **Ctrl + Enter** on Windows). 

The inference results should show a short, so-called **text-generation** response to the prompt in our query.

Next, try again with a larger **max_new_tokens** value. Change its value to 150, and then run the steps above again. 
Notice that there is a different, less "hallucinated", fuller generated response than last time.

You can also try experimenting with different **prompts** for this **text-generation** task.
Notice that the current prompt is `Databricks is` - supplied to the **pgml.transform** function's _input_ argument.
Try using other prompts that the LLM will attempt to complete. Replace `Databricks is` with another partially complete phrase, and repeat the steps above again.

